Traceback (most recent call last):
  File "/public/home/wangar2023/CS182-Final-Project/src/mask_autoencoder/v4.py", line 1395, in <module>
    history, model_path = resume_training_from_checkpoint(
  File "/public/home/wangar2023/CS182-Final-Project/src/mask_autoencoder/v4.py", line 1146, in resume_training_from_checkpoint
    model.load_state_dict(model_state_dict)
  File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for ProteinInteractionClassifier:
	Missing key(s) in state_dict: "protein_encoder.layers.8.self_attn.in_proj_weight", "protein_encoder.layers.8.self_attn.in_proj_bias", "protein_encoder.layers.8.self_attn.out_proj.weight", "protein_encoder.layers.8.self_attn.out_proj.bias", "protein_encoder.layers.8.ffn.0.weight", "protein_encoder.layers.8.ffn.0.bias", "protein_encoder.layers.8.ffn.3.weight", "protein_encoder.layers.8.ffn.3.bias", "protein_encoder.layers.8.norm1.weight", "protein_encoder.layers.8.norm1.bias", "protein_encoder.layers.8.norm2.weight", "protein_encoder.layers.8.norm2.bias", "protein_encoder.layers.9.self_attn.in_proj_weight", "protein_encoder.layers.9.self_attn.in_proj_bias", "protein_encoder.layers.9.self_attn.out_proj.weight", "protein_encoder.layers.9.self_attn.out_proj.bias", "protein_encoder.layers.9.ffn.0.weight", "protein_encoder.layers.9.ffn.0.bias", "protein_encoder.layers.9.ffn.3.weight", "protein_encoder.layers.9.ffn.3.bias", "protein_encoder.layers.9.norm1.weight", "protein_encoder.layers.9.norm1.bias", "protein_encoder.layers.9.norm2.weight", "protein_encoder.layers.9.norm2.bias", "protein_encoder.layers.10.self_attn.in_proj_weight", "protein_encoder.layers.10.self_attn.in_proj_bias", "protein_encoder.layers.10.self_attn.out_proj.weight", "protein_encoder.layers.10.self_attn.out_proj.bias", "protein_encoder.layers.10.ffn.0.weight", "protein_encoder.layers.10.ffn.0.bias", "protein_encoder.layers.10.ffn.3.weight", "protein_encoder.layers.10.ffn.3.bias", "protein_encoder.layers.10.norm1.weight", "protein_encoder.layers.10.norm1.bias", "protein_encoder.layers.10.norm2.weight", "protein_encoder.layers.10.norm2.bias", "protein_encoder.layers.11.self_attn.in_proj_weight", "protein_encoder.layers.11.self_attn.in_proj_bias", "protein_encoder.layers.11.self_attn.out_proj.weight", "protein_encoder.layers.11.self_attn.out_proj.bias", "protein_encoder.layers.11.ffn.0.weight", "protein_encoder.layers.11.ffn.0.bias", "protein_encoder.layers.11.ffn.3.weight", "protein_encoder.layers.11.ffn.3.bias", "protein_encoder.layers.11.norm1.weight", "protein_encoder.layers.11.norm1.bias", "protein_encoder.layers.11.norm2.weight", "protein_encoder.layers.11.norm2.bias", "protein_encoder.layers.12.self_attn.in_proj_weight", "protein_encoder.layers.12.self_attn.in_proj_bias", "protein_encoder.layers.12.self_attn.out_proj.weight", "protein_encoder.layers.12.self_attn.out_proj.bias", "protein_encoder.layers.12.ffn.0.weight", "protein_encoder.layers.12.ffn.0.bias", "protein_encoder.layers.12.ffn.3.weight", "protein_encoder.layers.12.ffn.3.bias", "protein_encoder.layers.12.norm1.weight", "protein_encoder.layers.12.norm1.bias", "protein_encoder.layers.12.norm2.weight", "protein_encoder.layers.12.norm2.bias", "protein_encoder.layers.13.self_attn.in_proj_weight", "protein_encoder.layers.13.self_attn.in_proj_bias", "protein_encoder.layers.13.self_attn.out_proj.weight", "protein_encoder.layers.13.self_attn.out_proj.bias", "protein_encoder.layers.13.ffn.0.weight", "protein_encoder.layers.13.ffn.0.bias", "protein_encoder.layers.13.ffn.3.weight", "protein_encoder.layers.13.ffn.3.bias", "protein_encoder.layers.13.norm1.weight", "protein_encoder.layers.13.norm1.bias", "protein_encoder.layers.13.norm2.weight", "protein_encoder.layers.13.norm2.bias", "protein_encoder.layers.14.self_attn.in_proj_weight", "protein_encoder.layers.14.self_attn.in_proj_bias", "protein_encoder.layers.14.self_attn.out_proj.weight", "protein_encoder.layers.14.self_attn.out_proj.bias", "protein_encoder.layers.14.ffn.0.weight", "protein_encoder.layers.14.ffn.0.bias", "protein_encoder.layers.14.ffn.3.weight", "protein_encoder.layers.14.ffn.3.bias", "protein_encoder.layers.14.norm1.weight", "protein_encoder.layers.14.norm1.bias", "protein_encoder.layers.14.norm2.weight", "protein_encoder.layers.14.norm2.bias", "protein_encoder.layers.15.self_attn.in_proj_weight", "protein_encoder.layers.15.self_attn.in_proj_bias", "protein_encoder.layers.15.self_attn.out_proj.weight", "protein_encoder.layers.15.self_attn.out_proj.bias", "protein_encoder.layers.15.ffn.0.weight", "protein_encoder.layers.15.ffn.0.bias", "protein_encoder.layers.15.ffn.3.weight", "protein_encoder.layers.15.ffn.3.bias", "protein_encoder.layers.15.norm1.weight", "protein_encoder.layers.15.norm1.bias", "protein_encoder.layers.15.norm2.weight", "protein_encoder.layers.15.norm2.bias". 
	size mismatch for decoder.layers.0.linear.weight: copying a param with shape torch.Size([256, 960]) from checkpoint, the shape in current model is torch.Size([512, 960]).
	size mismatch for decoder.layers.0.linear.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for decoder.layers.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for decoder.layers.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for decoder.layers.0.residual.weight: copying a param with shape torch.Size([256, 960]) from checkpoint, the shape in current model is torch.Size([512, 960]).
	size mismatch for decoder.layers.0.residual.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for decoder.layers.1.linear.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for decoder.layers.1.linear.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for decoder.layers.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for decoder.layers.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for decoder.layers.1.residual.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for decoder.layers.1.residual.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for decoder.layers.2.linear.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).
	size mismatch for decoder.layers.2.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for decoder.layers.2.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for decoder.layers.2.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for decoder.layers.2.residual.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).
	size mismatch for decoder.layers.2.residual.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for decoder.final_layer.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 128]).
