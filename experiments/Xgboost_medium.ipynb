{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression on medium set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import gc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Examining training data structure:\n",
      "DataFrame columns: ['uniprotID_A', 'uniprotID_B', 'isInteraction', 'trainTest', 'sequence_A', 'sequence_B']\n",
      "First row sample: {'uniprotID_A': 'Q92529', 'uniprotID_B': 'Q9H6L4', 'isInteraction': 0, 'trainTest': 'train', 'sequence_A': 'MLPRTKYNRFRNDSVTSVDDLLHSLSVSGGGGKVSAARATPAAAPYLVSGEALRKAPDDGPGSLGHLLHKVSHLKLSSSGLRGLSSAARERAGARLSGSCSAPSLAAPDGSAPSAPRAPAMSAARKGRPGDEPLPRPPRGAPHASDQVLGPGVTYVVKYLGCIEVLRSMRSLDFSTRTQITREAISRVCEAVPGAKGAFKKRKPPSKMLSSILGKSNLQFAGMSISLTISTASLNLRTPDSKQIIANHHMRSISFASGGDPDTTDYVAYVAKDPVNRRACHILECCDGLAQDVIGSIGQAFELRFKQYLQCPTKIPALHDRMQSLDEPWTEEEGDGSDHPYYNSIPSKMPPPGGFLDTRLKPRPHAPDTAQFAGKEQTYYQGRHLGDTFGEDWQQTPLRQGSSDIYSTPEGKLHVAPTGEAPTYVNTQQIPPQAWPAAVSSAESSPRKDLFDMKPFEDALKNQPLGPVLSKAASVECISPVSPRAPDAKMLEELQAETWYQGEMSRKEAEGLLEKDGDFLVRKSTTNPGSFVLTGMHNGQAKHLLLVDPEGTIRTKDRVFDSISHLINHHLESSLPIVSAGSELCLQQPVERKQ', 'sequence_B': 'MAQKPKVDPHVGRLGYLQALVTEFQETQSQDAKEQVLANLANFAYDPSNYEYLRQLQVLDLFLDSLSEENETLVEFAIGGLCNLCPDRANKEHILHAGGVPLIINCLSSPNEETVLSAITTLMHLSPPGRSFLPELTATPVVQCMLRFSLSASARLRNLAQIFLEDFCSPRQVAEARSRQAHSALGIPLPRSVAPRQR'}\n",
      "\n",
      "Loading protein embeddings...\n",
      "Loaded 12026 protein embeddings\n",
      "  uniprotID_A uniprotID_B  isInteraction trainTest  \\\n",
      "0      Q92529      Q9H6L4              0     train   \n",
      "1      P09326      Q02446              0     train   \n",
      "2      Q4V328      Q9H2H9              0     train   \n",
      "3      O95835      Q9NTJ5              0     train   \n",
      "4      O95125      Q96JC9              1     train   \n",
      "\n",
      "                                          sequence_A  \\\n",
      "0  MLPRTKYNRFRNDSVTSVDDLLHSLSVSGGGGKVSAARATPAAAPY...   \n",
      "1  MCSRGWDSCLALELLLLPLSLLVTSIQGHLVHMTVVSGSNVTLNIS...   \n",
      "2  MAQALSEEEFQRMQAQLLELRTNNYQLSDELRKNGVELTSLRQKVA...   \n",
      "3  MKRSEKPEGYRQMRPKTFPASNYTVSSRQMLQEIRESLRNLSKPSD...   \n",
      "4  MATAVEPEDQDLWEEEGILMVKLEDDFTCRPESVLQRDDPVLETSH...   \n",
      "\n",
      "                                          sequence_B  \n",
      "0  MAQKPKVDPHVGRLGYLQALVTEFQETQSQDAKEQVLANLANFAYD...  \n",
      "1  MSDQKKEEEEEAAAAAAMATEGGKTSEPENNNKKPKTSGSQDSQPS...  \n",
      "2  MMHFKSGLELTELQNMTVPEDDNISNDSNDFTEVENGQINSKFISD...  \n",
      "3  MATAAYEQLKLHITPEKFYVEACDDGADDVLTIDRVSTEVTLAVKK...  \n",
      "4  MNGTANPLLDREEHCLRLGESFEKRPRASFHTIRYDFKPASIDTSC...  \n",
      "Protein ID: O14796, Embedding shape: torch.Size([134, 960])\n",
      "Protein ID: Q8IYI6, Embedding shape: torch.Size([727, 960])\n",
      "Protein ID: Q8TBN0, Embedding shape: torch.Size([384, 960])\n",
      "Protein ID: Q8N3R9, Embedding shape: torch.Size([677, 960])\n",
      "Protein ID: Q9UI14, Embedding shape: torch.Size([187, 960])\n"
     ]
    }
   ],
   "source": [
    "# First, let's examine the data structure to identify column names\n",
    "def examine_dataframe(df):\n",
    "    \"\"\"Print the structure of the dataframe to identify column names\"\"\"\n",
    "    print(\"DataFrame columns:\", df.columns.tolist())\n",
    "    print(\"First row sample:\", df.iloc[0].to_dict())\n",
    "    return df.columns.tolist()\n",
    "\n",
    "\n",
    "# Get the absolute path of the current notebook\n",
    "notebook_path = os.path.abspath('')\n",
    "\n",
    "# Navigate to the project root (CS182-Final-Project)\n",
    "project_root = os.path.dirname(notebook_path)\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_data = pickle.load(open('data/medium_set/train_data.pkl', 'rb'))\n",
    "cv_data = pickle.load(open('data/medium_set/validation_data.pkl', 'rb'))\n",
    "test1_data = pickle.load(open('data/medium_set/test1_data.pkl', 'rb'))\n",
    "test2_data = pickle.load(open('data/medium_set/test2_data.pkl', 'rb'))\n",
    "\n",
    "# Examine structure of the first dataframe to understand its format\n",
    "print(\"\\nExamining training data structure:\")\n",
    "examine_dataframe(train_data)\n",
    "\n",
    "print(\"\\nLoading protein embeddings...\")\n",
    "protein_embeddings = pickle.load(open('data/full_dataset/embeddings/embeddings_merged.pkl', 'rb'))\n",
    "print(f\"Loaded {len(protein_embeddings)} protein embeddings\")\n",
    "\n",
    "print(train_data.head())\n",
    "for i, (key, value) in enumerate(protein_embeddings.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Protein ID: {key}, Embedding shape: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from training data...\n",
      "Using columns: Protein A = 'uniprotID_A', Protein B = 'uniprotID_B', Interaction = 'isInteraction'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 100/100 [00:17<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 protein pairs. Skipped 0 pairs (not found in embeddings).\n",
      "\n",
      "Extracting features from validation data...\n",
      "Using columns: Protein A = 'uniprotID_A', Protein B = 'uniprotID_B', Interaction = 'isInteraction'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 20/20 [00:03<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 protein pairs. Skipped 0 pairs (not found in embeddings).\n",
      "\n",
      "Scaling features...\n"
     ]
    }
   ],
   "source": [
    "# Function to extract features using correct column names\n",
    "def extract_features_batch(data_df, embeddings_dict, batch_size=100):\n",
    "    \"\"\"Process data in batches and extract features by combining protein embeddings\"\"\"\n",
    "    # First examine the structure to get correct column names\n",
    "    columns = data_df.columns.tolist()\n",
    "    \n",
    "    # Determine protein ID and interaction columns\n",
    "    protein_a_col = None\n",
    "    protein_b_col = None\n",
    "    interaction_col = None\n",
    "    \n",
    "    # Common column name patterns\n",
    "    protein_a_patterns = ['protein_a', 'protein_id_a', 'proteinA', 'proteinIDA', 'protein_A', 'protein_id_A']\n",
    "    protein_b_patterns = ['protein_b', 'protein_id_b', 'proteinB', 'proteinIDB', 'protein_B', 'protein_id_B']\n",
    "    interaction_patterns = ['isInteraction', 'is_interaction', 'interaction', 'label']\n",
    "    \n",
    "    # Find protein ID columns\n",
    "    for col in columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(pattern.lower() in col_lower for pattern in protein_a_patterns):\n",
    "            protein_a_col = col\n",
    "        elif any(pattern.lower() in col_lower for pattern in protein_b_patterns):\n",
    "            protein_b_col = col\n",
    "        elif any(pattern.lower() in col_lower for pattern in interaction_patterns):\n",
    "            interaction_col = col\n",
    "    \n",
    "    # If we still can't find the columns, look for any that might contain protein IDs\n",
    "    if protein_a_col is None or protein_b_col is None:\n",
    "        # Check the first row to see if any column contains values that match keys in embeddings_dict\n",
    "        first_row = data_df.iloc[0].to_dict()\n",
    "        for col, val in first_row.items():\n",
    "            if isinstance(val, str) and val in embeddings_dict:\n",
    "                if protein_a_col is None:\n",
    "                    protein_a_col = col\n",
    "                elif protein_b_col is None and col != protein_a_col:\n",
    "                    protein_b_col = col\n",
    "    \n",
    "    if protein_a_col is None or protein_b_col is None or interaction_col is None:\n",
    "        print(\"Column detection failed. Please specify column names manually.\")\n",
    "        print(\"Available columns:\", columns)\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Using columns: Protein A = '{protein_a_col}', Protein B = '{protein_b_col}', Interaction = '{interaction_col}'\")\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    skipped = 0\n",
    "    \n",
    "    # Process in batches to save memory\n",
    "    for i in tqdm(range(0, len(data_df), batch_size), desc=\"Extracting features\"):\n",
    "        batch = data_df.iloc[i:i+batch_size]\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            # Get protein IDs\n",
    "            protein_A = row[protein_a_col]\n",
    "            protein_B = row[protein_b_col]\n",
    "            \n",
    "            # Skip if embeddings are not available\n",
    "            if protein_A not in embeddings_dict or protein_B not in embeddings_dict:\n",
    "                skipped += 1\n",
    "                if skipped < 5:  # Print first few skipped to debug\n",
    "                    print(f\"Skipping pair: {protein_A}, {protein_B} - not found in embeddings\")\n",
    "                continue\n",
    "            \n",
    "            # Get embeddings\n",
    "            embedding_A = embeddings_dict[protein_A]\n",
    "            embedding_B = embeddings_dict[protein_B]\n",
    "            \n",
    "            # Mean pooling over sequence length to get a fixed-size representation\n",
    "            if isinstance(embedding_A, torch.Tensor):\n",
    "                feat_A = embedding_A.mean(dim=0).cpu().numpy()\n",
    "                feat_B = embedding_B.mean(dim=0).cpu().numpy()\n",
    "            else:\n",
    "                feat_A = embedding_A.mean(axis=0)\n",
    "                feat_B = embedding_B.mean(axis=0)\n",
    "            \n",
    "            # Combine features (concatenation)\n",
    "            combined_features = np.concatenate([feat_A, feat_B])\n",
    "            \n",
    "            batch_X.append(combined_features)\n",
    "            batch_y.append(row[interaction_col])\n",
    "        \n",
    "        # Add batch to full dataset\n",
    "        X.extend(batch_X)\n",
    "        y.extend(batch_y)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del batch_X, batch_y\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Processed {len(X)} protein pairs. Skipped {skipped} pairs (not found in embeddings).\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Extract features\n",
    "print(\"\\nExtracting features from training data...\")\n",
    "X_train, y_train = extract_features_batch(train_data, protein_embeddings)\n",
    "\n",
    "if X_train is None:  # Exit if feature extraction failed\n",
    "    print(\"Feature extraction failed. Please check the dataframe structure.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nExtracting features from validation data...\")\n",
    "X_cv, y_cv = extract_features_batch(cv_data, protein_embeddings)\n",
    "\n",
    "# Scale features\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_cv_scaled = scaler.transform(X_cv)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing create_features function...\n",
      "You already have scaled features. Let's extract embeddings for XGBoost...\n",
      "Extracting separate embeddings for XGBoost...\n",
      "Using columns: Protein A = 'uniprotID_A', Protein B = 'uniprotID_B', Interaction = 'isInteraction'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 100/100 [00:04<00:00, 23.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 protein pairs. Skipped 0 pairs.\n",
      "Using columns: Protein A = 'uniprotID_A', Protein B = 'uniprotID_B', Interaction = 'isInteraction'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 20/20 [00:01<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 protein pairs. Skipped 0 pairs.\n",
      "Using columns: Protein A = 'uniprotID_A', Protein B = 'uniprotID_B', Interaction = 'isInteraction'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 20/20 [00:01<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 protein pairs. Skipped 0 pairs.\n",
      "Using columns: Protein A = 'uniprotID_A', Protein B = 'uniprotID_B', Interaction = 'isInteraction'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 100/100 [00:04<00:00, 20.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 protein pairs. Skipped 0 pairs.\n",
      "Train embeddings: A=(10000, 960), B=(10000, 960), y=(10000,)\n",
      "Val embeddings: A=(2000, 960), B=(2000, 960), y=(2000,)\n",
      "Test1 embeddings: A=(2000, 960), B=(2000, 960), y=(2000,)\n",
      "Test2 embeddings: A=(10000, 960), B=(10000, 960), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "def create_features(embA, embB, method='all'):\n",
    "    \"\"\"Create combined features from protein embeddings\"\"\"\n",
    "    if method == 'all':\n",
    "        return np.concatenate([embA, embB, np.abs(embA - embB), embA * embB], axis=1)\n",
    "    elif method == 'add_sub':\n",
    "        return np.concatenate([embA + embB, embA - embB], axis=1)\n",
    "    elif method == 'mul_only':\n",
    "        return embA * embB\n",
    "    elif method == 'concatenate':\n",
    "        return np.concatenate([embA, embB], axis=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "# Test the function with your existing data\n",
    "print(\"Testing create_features function...\")\n",
    "if 'X_train_scaled' in locals():\n",
    "    # If you already have scaled features, let's work with the raw embeddings\n",
    "    # We need to extract embeddings first\n",
    "    print(\"You already have scaled features. Let's extract embeddings for XGBoost...\")\n",
    "\n",
    "# Since you already ran extract_features_batch, let's extract the individual embeddings\n",
    "# We need to modify the extract_features_batch to return separate embeddings\n",
    "\n",
    "def extract_embeddings_separate(data_df, embeddings_dict, batch_size=100):\n",
    "    \"\"\"Extract separate embeddings for protein A and B\"\"\"\n",
    "    columns = data_df.columns.tolist()\n",
    "    \n",
    "    # Find column names (reuse your existing logic)\n",
    "    protein_a_col = None\n",
    "    protein_b_col = None\n",
    "    interaction_col = None\n",
    "    \n",
    "    protein_a_patterns = ['protein_a', 'protein_id_a', 'proteinA', 'proteinIDA', 'protein_A', 'protein_id_A', 'uniprotID_A']\n",
    "    protein_b_patterns = ['protein_b', 'protein_id_b', 'proteinB', 'proteinIDB', 'protein_B', 'protein_id_B', 'uniprotID_B']\n",
    "    interaction_patterns = ['isInteraction', 'is_interaction', 'interaction', 'label']\n",
    "    \n",
    "    for col in columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(pattern.lower() in col_lower for pattern in protein_a_patterns):\n",
    "            protein_a_col = col\n",
    "        elif any(pattern.lower() in col_lower for pattern in protein_b_patterns):\n",
    "            protein_b_col = col\n",
    "        elif any(pattern.lower() in col_lower for pattern in interaction_patterns):\n",
    "            interaction_col = col\n",
    "    \n",
    "    if protein_a_col is None or protein_b_col is None:\n",
    "        first_row = data_df.iloc[0].to_dict()\n",
    "        for col, val in first_row.items():\n",
    "            if isinstance(val, str) and val in embeddings_dict:\n",
    "                if protein_a_col is None:\n",
    "                    protein_a_col = col\n",
    "                elif protein_b_col is None and col != protein_a_col:\n",
    "                    protein_b_col = col\n",
    "    \n",
    "    print(f\"Using columns: Protein A = '{protein_a_col}', Protein B = '{protein_b_col}', Interaction = '{interaction_col}'\")\n",
    "    \n",
    "    embA_list = []\n",
    "    embB_list = []\n",
    "    y_list = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(data_df), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch = data_df.iloc[i:i+batch_size]\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            protein_A = row[protein_a_col]\n",
    "            protein_B = row[protein_b_col]\n",
    "            \n",
    "            if protein_A not in embeddings_dict or protein_B not in embeddings_dict:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            embedding_A = embeddings_dict[protein_A]\n",
    "            embedding_B = embeddings_dict[protein_B]\n",
    "            \n",
    "            # Mean pooling\n",
    "            if isinstance(embedding_A, torch.Tensor):\n",
    "                if embedding_A.dim() == 3:  # Handle (1, seq_len, dim)\n",
    "                    feat_A = embedding_A[0].mean(dim=0).cpu().numpy()\n",
    "                    feat_B = embedding_B[0].mean(dim=0).cpu().numpy()\n",
    "                else:  # Handle (seq_len, dim)\n",
    "                    feat_A = embedding_A.mean(dim=0).cpu().numpy()\n",
    "                    feat_B = embedding_B.mean(dim=0).cpu().numpy()\n",
    "            else:\n",
    "                if len(embedding_A.shape) == 3:\n",
    "                    feat_A = embedding_A[0].mean(axis=0)\n",
    "                    feat_B = embedding_B[0].mean(axis=0)\n",
    "                else:\n",
    "                    feat_A = embedding_A.mean(axis=0)\n",
    "                    feat_B = embedding_B.mean(axis=0)\n",
    "            \n",
    "            embA_list.append(feat_A)\n",
    "            embB_list.append(feat_B)\n",
    "            y_list.append(row[interaction_col])\n",
    "    \n",
    "    print(f\"Processed {len(embA_list)} protein pairs. Skipped {skipped} pairs.\")\n",
    "    return np.array(embA_list), np.array(embB_list), np.array(y_list)\n",
    "\n",
    "# Extract separate embeddings for all datasets\n",
    "print(\"Extracting separate embeddings for XGBoost...\")\n",
    "train_embA, train_embB, train_y = extract_embeddings_separate(train_data, protein_embeddings)\n",
    "val_embA, val_embB, val_y = extract_embeddings_separate(cv_data, protein_embeddings)\n",
    "test1_embA, test1_embB, test1_y = extract_embeddings_separate(test1_data, protein_embeddings)\n",
    "test2_embA, test2_embB, test2_y = extract_embeddings_separate(test2_data, protein_embeddings)\n",
    "\n",
    "print(f\"Train embeddings: A={train_embA.shape}, B={train_embB.shape}, y={train_y.shape}\")\n",
    "print(f\"Val embeddings: A={val_embA.shape}, B={val_embB.shape}, y={val_y.shape}\")\n",
    "print(f\"Test1 embeddings: A={test1_embA.shape}, B={test1_embB.shape}, y={test1_y.shape}\")\n",
    "print(f\"Test2 embeddings: A={test2_embA.shape}, B={test2_embB.shape}, y={test2_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing method: all\n",
      "X_train shape: (10000, 3840)\n",
      "X_val shape: (2000, 3840)\n",
      "train_y shape: (10000,)\n",
      "val_y shape: (2000,)\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/miniconda3/envs/esm/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:28:00] WARNING: /croot/xgboost-split_1724073744422/work/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (all): 0.5995\n",
      "Validation AUC (all): 0.6374\n",
      "Validation F1 (all): 0.6029\n",
      "Validation Precision (all): 0.5978\n",
      "Validation Recall (all): 0.6080\n",
      "\n",
      "Best method: all with accuracy: 0.5995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Your existing XGBoost parameters\n",
    "methods = ['all']  # You can add 'add_sub', 'mul_only' later\n",
    "best_score = 0\n",
    "best_model = None\n",
    "best_method = None\n",
    "best_scaler = None\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 80,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "    'device': 'cuda'\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nTesting method: {method}\")\n",
    "    \n",
    "    # Create features using your existing function structure\n",
    "    X_train = create_features(train_embA, train_embB, method=method)\n",
    "    X_val = create_features(val_embA, val_embB, method=method)\n",
    "    X_test1 = create_features(test1_embA, test1_embB, method=method)\n",
    "    X_test2 = create_features(test2_embA, test2_embB, method=method)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"train_y shape: {train_y.shape}\")\n",
    "    print(f\"val_y shape: {val_y.shape}\")\n",
    "    \n",
    "    # Scale features manually (without Pipeline)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test1_scaled = scaler.transform(X_test1)\n",
    "    X_test2_scaled = scaler.transform(X_test2)\n",
    "    \n",
    "    # Create and fit XGBoost model directly\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "    \n",
    "    # Fit the model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model.fit(X_train_scaled, train_y)\n",
    "    \n",
    "    # Validation predictions\n",
    "    val_preds = xgb_model.predict(X_val_scaled)\n",
    "    val_proba = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_acc = accuracy_score(val_y, val_preds)\n",
    "    val_auc = roc_auc_score(val_y, val_proba)\n",
    "    val_f1 = f1_score(val_y, val_preds)\n",
    "    val_precision = precision_score(val_y, val_preds)\n",
    "    val_recall = recall_score(val_y, val_preds)\n",
    "    \n",
    "    print(f\"Validation Accuracy ({method}): {val_acc:.4f}\")\n",
    "    print(f\"Validation AUC ({method}): {val_auc:.4f}\")\n",
    "    print(f\"Validation F1 ({method}): {val_f1:.4f}\")\n",
    "    print(f\"Validation Precision ({method}): {val_precision:.4f}\")\n",
    "    print(f\"Validation Recall ({method}): {val_recall:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[method] = {\n",
    "        'accuracy': val_acc,\n",
    "        'auc': val_auc,\n",
    "        'f1': val_f1,\n",
    "        'precision': val_precision,\n",
    "        'recall': val_recall,\n",
    "        'model': xgb_model,\n",
    "        'scaler': scaler,\n",
    "        'X_train_scaled': X_train_scaled,\n",
    "        'X_val_scaled': X_val_scaled,\n",
    "        'X_test1_scaled': X_test1_scaled,\n",
    "        'X_test2_scaled': X_test2_scaled\n",
    "    }\n",
    "\n",
    "    if val_acc > best_score:\n",
    "        best_score = val_acc\n",
    "        best_model = xgb_model\n",
    "        best_method = method\n",
    "        best_scaler = scaler\n",
    "\n",
    "print(f\"\\nBest method: {best_method} with accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEST SET EVALUATION\n",
      "==================================================\n",
      "Test1 Accuracy: 0.5725\n",
      "Test1 AUC: 0.6104\n",
      "Test1 F1 Score: 0.5799\n",
      "Test1 Precision: 0.5700\n",
      "Test1 Recall: 0.5900\n",
      "\n",
      "Test2 Accuracy: 0.5657\n",
      "Test2 AUC: 0.6197\n",
      "Test2 F1 Score: 0.1983\n",
      "Test2 Precision: 0.1189\n",
      "Test2 Recall: 0.5967\n",
      "\n",
      "==================== Test1 Classification Report ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.56      1000\n",
      "           1       0.57      0.59      0.58      1000\n",
      "\n",
      "    accuracy                           0.57      2000\n",
      "   macro avg       0.57      0.57      0.57      2000\n",
      "weighted avg       0.57      0.57      0.57      2000\n",
      "\n",
      "\n",
      "==================== Test2 Classification Report ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.56      0.70      9100\n",
      "           1       0.12      0.60      0.20       900\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.53      0.58      0.45     10000\n",
      "weighted avg       0.86      0.57      0.66     10000\n",
      "\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Best Method: all\n",
      "Validation Accuracy: 0.5995\n",
      "Test1 Accuracy: 0.5725\n",
      "Test2 Accuracy: 0.5657\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test on test datasets\n",
    "best_results = results[best_method]\n",
    "best_xgb_model = best_results['model']\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test1 evaluation\n",
    "test1_preds = best_xgb_model.predict(best_results['X_test1_scaled'])\n",
    "test1_proba = best_xgb_model.predict_proba(best_results['X_test1_scaled'])[:, 1]\n",
    "\n",
    "test1_acc = accuracy_score(test1_y, test1_preds)\n",
    "test1_auc = roc_auc_score(test1_y, test1_proba)\n",
    "test1_f1 = f1_score(test1_y, test1_preds)\n",
    "test1_precision = precision_score(test1_y, test1_preds)\n",
    "test1_recall = recall_score(test1_y, test1_preds)\n",
    "\n",
    "print(f\"Test1 Accuracy: {test1_acc:.4f}\")\n",
    "print(f\"Test1 AUC: {test1_auc:.4f}\")\n",
    "print(f\"Test1 F1 Score: {test1_f1:.4f}\")\n",
    "print(f\"Test1 Precision: {test1_precision:.4f}\")\n",
    "print(f\"Test1 Recall: {test1_recall:.4f}\")\n",
    "\n",
    "# Test2 evaluation\n",
    "test2_preds = best_xgb_model.predict(best_results['X_test2_scaled'])\n",
    "test2_proba = best_xgb_model.predict_proba(best_results['X_test2_scaled'])[:, 1]\n",
    "\n",
    "test2_acc = accuracy_score(test2_y, test2_preds)\n",
    "test2_auc = roc_auc_score(test2_y, test2_proba)\n",
    "test2_f1 = f1_score(test2_y, test2_preds)\n",
    "test2_precision = precision_score(test2_y, test2_preds)\n",
    "test2_recall = recall_score(test2_y, test2_preds)\n",
    "\n",
    "print(f\"\\nTest2 Accuracy: {test2_acc:.4f}\")\n",
    "print(f\"Test2 AUC: {test2_auc:.4f}\")\n",
    "print(f\"Test2 F1 Score: {test2_f1:.4f}\")\n",
    "print(f\"Test2 Precision: {test2_precision:.4f}\")\n",
    "print(f\"Test2 Recall: {test2_recall:.4f}\")\n",
    "\n",
    "# Print classification reports\n",
    "print(f\"\\n{'='*20} Test1 Classification Report {'='*20}\")\n",
    "print(classification_report(test1_y, test1_preds))\n",
    "\n",
    "print(f\"\\n{'='*20} Test2 Classification Report {'='*20}\")\n",
    "print(classification_report(test2_y, test2_preds))\n",
    "\n",
    "# Summary\n",
    "results_summary = {\n",
    "    'best_method': best_method,\n",
    "    'best_score': best_score,\n",
    "    'validation_metrics': results[best_method],\n",
    "    'test1_metrics': {\n",
    "        'accuracy': test1_acc,\n",
    "        'auc': test1_auc,\n",
    "        'f1': test1_f1,\n",
    "        'precision': test1_precision,\n",
    "        'recall': test1_recall\n",
    "    },\n",
    "    'test2_metrics': {\n",
    "        'accuracy': test2_acc,\n",
    "        'auc': test2_auc,\n",
    "        'f1': test2_f1,\n",
    "        'precision': test2_precision,\n",
    "        'recall': test2_recall\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Method: {best_method}\")\n",
    "print(f\"Validation Accuracy: {results[best_method]['accuracy']:.4f}\")\n",
    "print(f\"Test1 Accuracy: {test1_acc:.4f}\")\n",
    "print(f\"Test2 Accuracy: {test2_acc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
